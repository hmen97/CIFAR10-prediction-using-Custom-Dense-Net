{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of Internshala Assignment DNST.ipynb","version":"0.3.2","provenance":[{"file_id":"1W_Kc-Yx4pS5OnjpR8h8CLkuDsNMUB2z4","timestamp":1553863126864},{"file_id":"1VNWlXxbuq085SLDb-AKVOWZuBu8TWm12","timestamp":1552899508059},{"file_id":"1inr2WcLSC7M07_DTI0VLr5QiIQbHvLQJ","timestamp":1552374259313},{"file_id":"1pFn0wvWOKj93A4_-pjMxsxR96VDyN-NF","timestamp":1552374189737},{"file_id":"1_1kwmwgL7g94jI6BEtcgm-D2_AFk0zxK","timestamp":1519101209834}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"K70hAckqg0EA","colab_type":"code","colab":{}},"cell_type":"code","source":["# https://keras.io/\n","!pip install -q keras\n","import keras"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wVIx_KIigxPV","colab_type":"code","colab":{}},"cell_type":"code","source":["#import keras\n","from keras.datasets import cifar10\n","from keras.utils import np_utils\n","from keras.models import Model, Sequential\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Flatten, Input, AveragePooling2D, merge, Activation\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from keras.layers import Concatenate\n","from keras.optimizers import Adam\n","from keras import regularizers\n","from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n","from keras.layers import Dense, Dropout\n","import imgaug as ia\n","from imgaug import augmenters as iaa\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UNHw6luQg3gc","colab_type":"code","colab":{}},"cell_type":"code","source":["# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n","# backend\n","import tensorflow as tf\n","from keras import backend as k\n","\n","# Don't pre-allocate memory; allocate as-needed\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","# Create a session with the above options specified.\n","k.tensorflow_backend.set_session(tf.Session(config=config))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3TUeBdUelk9Y","colab_type":"code","colab":{}},"cell_type":"code","source":["ia.seed(1)\n","\n","sometimes = lambda aug: iaa.Sometimes(0.3, aug)\n","\n","# Define our sequence of augmentation steps that will be applied to every image.\n","aug_seq = iaa.Sequential([ iaa.Fliplr(0.5), # horizontally flip 50% of all images\n","        iaa.Flipud(0.2), # vertically flip 20% of all images\n","        # crop some of the images by 0-10% of their height/width\n","        sometimes(iaa.Crop(percent=(0, 0.1))),\n","        # Apply affine transformations to some of the images\n","        sometimes(iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n","        rotate=(-45, 45), shear=(-16, 16), order=[0, 1], cval=(0, 255), mode=ia.ALL)), iaa.SomeOf((0, 5),\n","        # Convert some images into their superpixel representation\n","        [sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))),\n","        # Blur each image with varying strength using\n","        iaa.OneOf([iaa.GaussianBlur((0, 3.0)),iaa.AverageBlur(k=(2, 7)), iaa.MedianBlur(k=(3, 11))]),\n","        # Sharpen each image, overlay the result with the original\n","         iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n","         # Same as sharpen, but for an embossing effect.\n","         iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n","         # Search in some images either for all edges or for directed edges.\n","         sometimes(iaa.OneOf([iaa.EdgeDetect(alpha=(0, 0.7)),iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0))])),\n","         # Add gaussian noise to some images.\n","         iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n","         # Either drop randomly 1 to 10% of all pixels (i.e. set them to black) or drop them on an image with 2-5% percent\n","         # of the original size, leading to large dropped rectangles.\n","         iaa.OneOf([iaa.Dropout((0.01, 0.1), per_channel=0.5),iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05),\n","        per_channel=0.2)]),\n","         # Invert each image's chanell with 5% probability.\n","         iaa.Invert(0.05, per_channel=True), # invert color channels\n","         # Add a value of -10 to 10 to each pixel.\n","         iaa.Add((-10, 10), per_channel=0.5),\n","         # Change brightness of images (50-150% of original value).\n","         iaa.Multiply((0.5, 1.5), per_channel=0.5),\n","         # Improve or worsen the contrast of images.\n","         iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n","         # Convert each image to grayscale and then overlay the\n","         iaa.Grayscale(alpha=(0.0, 1.0)),\n","         # In some images move pixels locally around (with random strengths).\n","         sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)),\n","         # In some images distort local areas with varying strength.\n","         sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05)))],random_order=True)], random_order=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8Lb5pGdgl07R","colab_type":"code","colab":{}},"cell_type":"code","source":["augidx = np.random.randint(0,50000, size=(20000)) #an array of 20k random indexes between (0,50k)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mB7o3zu1g6eT","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load CIFAR10 Data\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n","\n","x_aug = aug_seq.augment_images(x_train[augidx]) #generating 20k augmented images with augindexes\n","y_aug= y_train[augidx]\n","\n","x_train = np.append(x_train, x_aug, axis=0) #new training set consisting of augment images & labels\n","y_train = np.append(y_train, y_aug, axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aSeeV4-WmsN5","colab_type":"code","colab":{}},"cell_type":"code","source":["#convert integers to float; normalise and center the mean\n","x_train=x_train.astype(\"float32\")  \n","x_test=x_test.astype(\"float32\")\n","mean=np.mean(x_train)\n","std=np.std(x_train)\n","x_test=(x_test-mean)/std\n","x_train=(x_train-mean)/std\n","\n","num_classes = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wETHrCJEmuUO","colab_type":"code","colab":{}},"cell_type":"code","source":["# convert to one hot encoing \n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BDnJcuEyniGt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"5d827612-8d82-4c47-fcb5-b97e094dc23a","executionInfo":{"status":"ok","timestamp":1553841622172,"user_tz":-330,"elapsed":54744,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}}},"cell_type":"code","source":["print('shapes of x_train & y_train \\n',x_train.shape, y_train.shape)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["shapes of x_train & y_train \n"," (70000, 32, 32, 3) (70000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"_zAYMFfHv2WO","colab_type":"code","colab":{}},"cell_type":"code","source":["# Hyperparameters\n","batch_size = 128\n","#epochs = 125\n","num_filter = 64\n","compression = 0.5\n","dropout_rate = 0.2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ee-sge5Kg7vr","colab_type":"code","colab":{}},"cell_type":"code","source":["# Dense Block\n","def add_denseblock(input, num_filter, dropout_rate, layers, growth_rate, grow_nb_filters = False):\n","  \n","  global compression\n","  temp = input\n","  for _ in range(layers):\n","    \n","    BatchNorm = BatchNormalization()(temp)\n","    relu = Activation('relu')(BatchNorm)\n","\n","    Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3),\n","                        use_bias=False ,padding='same')(relu)\n","    if dropout_rate:\n","      Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n","    concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n","\n","    temp = concat\n","\n","    if grow_nb_filters:\n","      num_filter += growth_rate  \n","\n","  return temp, num_filter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OOP6IPsGhBwb","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_transition(input, num_filter, dropout_rate):\n","    global compression\n","    BatchNorm = BatchNormalization()(input)\n","    relu = Activation('relu')(BatchNorm)\n","    Conv2D_BottleNeck = Conv2D(int(num_filter*compression),\n","                               (1,1), use_bias=False ,padding='same')(relu)\n","    if dropout_rate>0:\n","      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n","    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n","    \n","    return avg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0RaKFpubhDIC","colab_type":"code","colab":{}},"cell_type":"code","source":["def output_layer(input):\n","    global compression\n","    BatchNorm = BatchNormalization()(input)\n","    relu = Activation('relu')(BatchNorm)\n","    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n","    flat = Flatten()(AvgPooling)\n","    output = Dense(num_classes, activation='softmax')(flat)\n","    \n","    return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"anPCpQWhhGb7","colab_type":"code","colab":{}},"cell_type":"code","source":["dropout_rate = 0.2\n","l = 12\n","growth_rate = 16\n","input = Input(shape=(img_height, img_width, channel,))\n","First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n","\n","First_Block, num_filter = add_denseblock(First_Conv2D, num_filter, dropout_rate + 0.1 , 8, growth_rate)\n","First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n","num_filter = int(num_filter * compression)\n","\n","\n","Second_Block, num_filter = add_denseblock(First_Transition, num_filter, dropout_rate, 6, growth_rate, True)\n","Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n","num_filter = int(num_filter * compression)\n","\n","\n","Third_Block, num_filter = add_denseblock(Second_Transition, num_filter, dropout_rate, 4, growth_rate)\n","Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n","num_filter = int(num_filter * compression)\n","\n","Last_Block, num_filter = add_denseblock(Third_Transition,  num_filter, dropout_rate, 4, growth_rate)\n","output = output_layer(Last_Block)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1kFh7pdxhNtT","colab_type":"code","outputId":"40fbcc12-2c83-4a34-fb05-c1cb50a2c628","executionInfo":{"status":"ok","timestamp":1553841626283,"user_tz":-330,"elapsed":45716,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}},"colab":{"base_uri":"https://localhost:8080/","height":4998}},"cell_type":"code","source":["model = Model(inputs=[input], outputs=[output])\n","model.summary()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 64)   1728        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 32)   18432       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_1[0][0]                   \n","                                                                 dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 96)   384         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 96)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 32)   27648       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 32, 32, 32)   0           conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 32, 32, 128)  0           concatenate_1[0][0]              \n","                                                                 dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 32)   36864       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 32, 160)  0           concatenate_2[0][0]              \n","                                                                 dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 32)   46080       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 32, 32, 32)   0           conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 32, 32, 192)  0           concatenate_3[0][0]              \n","                                                                 dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 32, 32, 192)  768         concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 32, 32, 192)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 32)   55296       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 32, 32, 32)   0           conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 32, 32, 224)  0           concatenate_4[0][0]              \n","                                                                 dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 32, 32, 224)  896         concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 32, 32, 224)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 32)   64512       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 32, 32, 32)   0           conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 32, 32, 256)  0           concatenate_5[0][0]              \n","                                                                 dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 32, 32, 32)   73728       activation_7[0][0]               \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 32, 32, 32)   0           conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_7 (Concatenate)     (None, 32, 32, 288)  0           concatenate_6[0][0]              \n","                                                                 dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 32, 32, 288)  1152        concatenate_7[0][0]              \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 32, 32, 288)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 32)   82944       activation_8[0][0]               \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 32, 32, 32)   0           conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_8 (Concatenate)     (None, 32, 32, 320)  0           concatenate_7[0][0]              \n","                                                                 dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 32, 320)  1280        concatenate_8[0][0]              \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 32, 32, 320)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 32)   10240       activation_9[0][0]               \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 32, 32, 32)   0           conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 16, 16, 32)   0           dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 16, 16, 16)   4608        activation_10[0][0]              \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 16, 16, 16)   0           conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 16, 16, 48)   0           average_pooling2d_1[0][0]        \n","                                                                 dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 16, 16, 48)   192         concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 16, 16, 48)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 16, 16, 24)   10368       activation_11[0][0]              \n","__________________________________________________________________________________________________\n","dropout_11 (Dropout)            (None, 16, 16, 24)   0           conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 16, 16, 72)   0           concatenate_9[0][0]              \n","                                                                 dropout_11[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 16, 16, 72)   288         concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 16, 16, 72)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 16, 16, 32)   20736       activation_12[0][0]              \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 16, 16, 32)   0           conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_11 (Concatenate)    (None, 16, 16, 104)  0           concatenate_10[0][0]             \n","                                                                 dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 16, 16, 104)  416         concatenate_11[0][0]             \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 16, 16, 104)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 16, 16, 40)   37440       activation_13[0][0]              \n","__________________________________________________________________________________________________\n","dropout_13 (Dropout)            (None, 16, 16, 40)   0           conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 16, 16, 144)  0           concatenate_11[0][0]             \n","                                                                 dropout_13[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 16, 16, 144)  576         concatenate_12[0][0]             \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 16, 16, 144)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 16, 16, 48)   62208       activation_14[0][0]              \n","__________________________________________________________________________________________________\n","dropout_14 (Dropout)            (None, 16, 16, 48)   0           conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_13 (Concatenate)    (None, 16, 16, 192)  0           concatenate_12[0][0]             \n","                                                                 dropout_14[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 16, 16, 192)  768         concatenate_13[0][0]             \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 16, 16, 192)  0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 16, 16, 56)   96768       activation_15[0][0]              \n","__________________________________________________________________________________________________\n","dropout_15 (Dropout)            (None, 16, 16, 56)   0           conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_14 (Concatenate)    (None, 16, 16, 248)  0           concatenate_13[0][0]             \n","                                                                 dropout_15[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 16, 16, 248)  992         concatenate_14[0][0]             \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 16, 16, 248)  0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 64)   15872       activation_16[0][0]              \n","__________________________________________________________________________________________________\n","dropout_16 (Dropout)            (None, 16, 16, 64)   0           conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","average_pooling2d_2 (AveragePoo (None, 8, 8, 64)     0           dropout_16[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         average_pooling2d_2[0][0]        \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 8, 8, 32)     18432       activation_17[0][0]              \n","__________________________________________________________________________________________________\n","dropout_17 (Dropout)            (None, 8, 8, 32)     0           conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_15 (Concatenate)    (None, 8, 8, 96)     0           average_pooling2d_2[0][0]        \n","                                                                 dropout_17[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 8, 8, 96)     384         concatenate_15[0][0]             \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 8, 8, 96)     0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 8, 8, 32)     27648       activation_18[0][0]              \n","__________________________________________________________________________________________________\n","dropout_18 (Dropout)            (None, 8, 8, 32)     0           conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_16 (Concatenate)    (None, 8, 8, 128)    0           concatenate_15[0][0]             \n","                                                                 dropout_18[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         concatenate_16[0][0]             \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 8, 8, 32)     36864       activation_19[0][0]              \n","__________________________________________________________________________________________________\n","dropout_19 (Dropout)            (None, 8, 8, 32)     0           conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_17 (Concatenate)    (None, 8, 8, 160)    0           concatenate_16[0][0]             \n","                                                                 dropout_19[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 8, 8, 160)    640         concatenate_17[0][0]             \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 8, 8, 160)    0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 8, 8, 32)     46080       activation_20[0][0]              \n","__________________________________________________________________________________________________\n","dropout_20 (Dropout)            (None, 8, 8, 32)     0           conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_18 (Concatenate)    (None, 8, 8, 192)    0           concatenate_17[0][0]             \n","                                                                 dropout_20[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 8, 192)    768         concatenate_18[0][0]             \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 8, 8, 192)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 8, 8, 32)     6144        activation_21[0][0]              \n","__________________________________________________________________________________________________\n","dropout_21 (Dropout)            (None, 8, 8, 32)     0           conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","average_pooling2d_3 (AveragePoo (None, 4, 4, 32)     0           dropout_21[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 4, 4, 32)     128         average_pooling2d_3[0][0]        \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 4, 4, 32)     0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 4, 4, 16)     4608        activation_22[0][0]              \n","__________________________________________________________________________________________________\n","dropout_22 (Dropout)            (None, 4, 4, 16)     0           conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_19 (Concatenate)    (None, 4, 4, 48)     0           average_pooling2d_3[0][0]        \n","                                                                 dropout_22[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 4, 4, 48)     192         concatenate_19[0][0]             \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 4, 4, 48)     0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 4, 4, 16)     6912        activation_23[0][0]              \n","__________________________________________________________________________________________________\n","dropout_23 (Dropout)            (None, 4, 4, 16)     0           conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 4, 4, 64)     0           concatenate_19[0][0]             \n","                                                                 dropout_23[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 4, 4, 64)     256         concatenate_20[0][0]             \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 4, 4, 64)     0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 4, 4, 16)     9216        activation_24[0][0]              \n","__________________________________________________________________________________________________\n","dropout_24 (Dropout)            (None, 4, 4, 16)     0           conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 4, 4, 80)     0           concatenate_20[0][0]             \n","                                                                 dropout_24[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 4, 4, 80)     320         concatenate_21[0][0]             \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 4, 4, 80)     0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 4, 4, 16)     11520       activation_25[0][0]              \n","__________________________________________________________________________________________________\n","dropout_25 (Dropout)            (None, 4, 4, 16)     0           conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 4, 4, 96)     0           concatenate_21[0][0]             \n","                                                                 dropout_25[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 4, 4, 96)     384         concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 4, 4, 96)     0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_4 (AveragePoo (None, 2, 2, 96)     0           activation_26[0][0]              \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 384)          0           average_pooling2d_4[0][0]        \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           3850        flatten_1[0][0]                  \n","==================================================================================================\n","Total params: 850,858\n","Trainable params: 843,802\n","Non-trainable params: 7,056\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"4aPjxygn4nFh","colab_type":"code","colab":{}},"cell_type":"code","source":["datagen = ImageDataGenerator(rotation_range=15,\n","                             horizontal_flip=True,\n","                             width_shift_range=0.1,\n","                             height_shift_range=0.1,\n","                             zoom_range=0.3\n","                             )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7NptFjdR5NOH","colab_type":"code","colab":{}},"cell_type":"code","source":["datagen.fit(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b4XOsW3ahSkL","colab_type":"code","colab":{}},"cell_type":"code","source":["# determine Loss function and Optimizer\n","opt_adam = keras.optimizers.Adam(lr=0.001,decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt_adam,\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"crhGk7kEhXAz","colab_type":"code","outputId":"09ef1c73-d8e6-4f3c-ec37-9607f5ea8f21","executionInfo":{"status":"ok","timestamp":1553804411075,"user_tz":-330,"elapsed":22430984,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}},"colab":{"base_uri":"https://localhost:8080/","height":2825}},"cell_type":"code","source":["model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size),\n","                    steps_per_epoch = x_train.shape[0] // batch_size,\n","                    epochs=80, verbose =1, validation_data =(x_test, y_test))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/80\n","546/546 [==============================] - 290s 531ms/step - loss: 1.6436 - acc: 0.4063 - val_loss: 1.3333 - val_acc: 0.5335\n","Epoch 2/80\n","546/546 [==============================] - 281s 514ms/step - loss: 1.3424 - acc: 0.5232 - val_loss: 1.5101 - val_acc: 0.5377\n","Epoch 3/80\n","546/546 [==============================] - 279s 511ms/step - loss: 1.1831 - acc: 0.5849 - val_loss: 0.9923 - val_acc: 0.6463\n","Epoch 4/80\n","546/546 [==============================] - 279s 511ms/step - loss: 1.0855 - acc: 0.6184 - val_loss: 1.1539 - val_acc: 0.6400\n","Epoch 5/80\n","546/546 [==============================] - 279s 510ms/step - loss: 1.0116 - acc: 0.6462 - val_loss: 0.7522 - val_acc: 0.7437\n","Epoch 6/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.9599 - acc: 0.6655 - val_loss: 0.7352 - val_acc: 0.7586\n","Epoch 7/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.9209 - acc: 0.6796 - val_loss: 0.7467 - val_acc: 0.7605\n","Epoch 8/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.8832 - acc: 0.6933 - val_loss: 0.7867 - val_acc: 0.7651\n","Epoch 9/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.8596 - acc: 0.7021 - val_loss: 1.1756 - val_acc: 0.6736\n","Epoch 10/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.8255 - acc: 0.7138 - val_loss: 0.6916 - val_acc: 0.7758\n","Epoch 11/80\n","546/546 [==============================] - 279s 512ms/step - loss: 0.8107 - acc: 0.7200 - val_loss: 0.6393 - val_acc: 0.8052\n","Epoch 12/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.7903 - acc: 0.7270 - val_loss: 0.6622 - val_acc: 0.7910\n","Epoch 13/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.7731 - acc: 0.7321 - val_loss: 0.7650 - val_acc: 0.7647\n","Epoch 14/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.7508 - acc: 0.7400 - val_loss: 0.5201 - val_acc: 0.8337\n","Epoch 15/80\n","546/546 [==============================] - 279s 512ms/step - loss: 0.7409 - acc: 0.7440 - val_loss: 0.6049 - val_acc: 0.8148\n","Epoch 16/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.7277 - acc: 0.7487 - val_loss: 0.5351 - val_acc: 0.8290\n","Epoch 17/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.7086 - acc: 0.7558 - val_loss: 0.5539 - val_acc: 0.8283\n","Epoch 18/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.7005 - acc: 0.7574 - val_loss: 0.5613 - val_acc: 0.8336\n","Epoch 19/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6889 - acc: 0.7638 - val_loss: 0.5253 - val_acc: 0.8372\n","Epoch 20/80\n","546/546 [==============================] - 279s 512ms/step - loss: 0.6833 - acc: 0.7635 - val_loss: 0.4887 - val_acc: 0.8447\n","Epoch 21/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6696 - acc: 0.7671 - val_loss: 0.4760 - val_acc: 0.8543\n","Epoch 22/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6604 - acc: 0.7732 - val_loss: 0.5948 - val_acc: 0.8283\n","Epoch 23/80\n","546/546 [==============================] - 279s 512ms/step - loss: 0.6560 - acc: 0.7733 - val_loss: 0.5526 - val_acc: 0.8393\n","Epoch 24/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.6440 - acc: 0.7776 - val_loss: 0.5556 - val_acc: 0.8306\n","Epoch 25/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6336 - acc: 0.7810 - val_loss: 0.4563 - val_acc: 0.8591\n","Epoch 26/80\n","546/546 [==============================] - 279s 512ms/step - loss: 0.6309 - acc: 0.7830 - val_loss: 0.4766 - val_acc: 0.8567\n","Epoch 27/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6194 - acc: 0.7873 - val_loss: 0.4672 - val_acc: 0.8623\n","Epoch 28/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.6168 - acc: 0.7862 - val_loss: 0.4720 - val_acc: 0.8611\n","Epoch 29/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.6083 - acc: 0.7882 - val_loss: 0.4188 - val_acc: 0.8712\n","Epoch 30/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.6001 - acc: 0.7941 - val_loss: 0.4691 - val_acc: 0.8629\n","Epoch 31/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.6044 - acc: 0.7896 - val_loss: 0.4716 - val_acc: 0.8585\n","Epoch 32/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5930 - acc: 0.7938 - val_loss: 0.4407 - val_acc: 0.8654\n","Epoch 33/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5840 - acc: 0.7992 - val_loss: 0.4239 - val_acc: 0.8673\n","Epoch 34/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5782 - acc: 0.8002 - val_loss: 0.4001 - val_acc: 0.8759\n","Epoch 35/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5775 - acc: 0.7984 - val_loss: 0.4197 - val_acc: 0.8724\n","Epoch 36/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.5718 - acc: 0.8025 - val_loss: 0.3782 - val_acc: 0.8850\n","Epoch 37/80\n","546/546 [==============================] - 280s 512ms/step - loss: 0.5678 - acc: 0.8047 - val_loss: 0.4126 - val_acc: 0.8765\n","Epoch 38/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5628 - acc: 0.8049 - val_loss: 0.5223 - val_acc: 0.8562\n","Epoch 39/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5629 - acc: 0.8058 - val_loss: 0.4054 - val_acc: 0.8757\n","Epoch 40/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5510 - acc: 0.8100 - val_loss: 0.4019 - val_acc: 0.8791\n","Epoch 41/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5557 - acc: 0.8077 - val_loss: 0.3813 - val_acc: 0.8865\n","Epoch 42/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5483 - acc: 0.8086 - val_loss: 0.4092 - val_acc: 0.8787\n","Epoch 43/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5432 - acc: 0.8112 - val_loss: 0.3610 - val_acc: 0.8916\n","Epoch 44/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5356 - acc: 0.8146 - val_loss: 0.4442 - val_acc: 0.8738\n","Epoch 45/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5382 - acc: 0.8125 - val_loss: 0.3851 - val_acc: 0.8842\n","Epoch 46/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5316 - acc: 0.8153 - val_loss: 0.3575 - val_acc: 0.8901\n","Epoch 47/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5271 - acc: 0.8184 - val_loss: 0.4004 - val_acc: 0.8767\n","Epoch 48/80\n","546/546 [==============================] - 280s 513ms/step - loss: 0.5275 - acc: 0.8158 - val_loss: 0.4256 - val_acc: 0.8787\n","Epoch 49/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5229 - acc: 0.8185 - val_loss: 0.3589 - val_acc: 0.8917\n","Epoch 50/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5214 - acc: 0.8183 - val_loss: 0.3509 - val_acc: 0.8944\n","Epoch 51/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.5189 - acc: 0.8191 - val_loss: 0.3976 - val_acc: 0.8845\n","Epoch 52/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.5156 - acc: 0.8198 - val_loss: 0.4034 - val_acc: 0.8794\n","Epoch 53/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.5100 - acc: 0.8226 - val_loss: 0.3442 - val_acc: 0.8978\n","Epoch 54/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.5112 - acc: 0.8210 - val_loss: 0.4097 - val_acc: 0.8797\n","Epoch 55/80\n","546/546 [==============================] - 280s 514ms/step - loss: 0.5057 - acc: 0.8234 - val_loss: 0.3361 - val_acc: 0.8973\n","Epoch 56/80\n","546/546 [==============================] - 281s 516ms/step - loss: 0.5052 - acc: 0.8244 - val_loss: 0.3503 - val_acc: 0.8913\n","Epoch 57/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.5007 - acc: 0.8271 - val_loss: 0.3558 - val_acc: 0.8950\n","Epoch 58/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.4935 - acc: 0.8286 - val_loss: 0.4227 - val_acc: 0.8788\n","Epoch 59/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.4978 - acc: 0.8260 - val_loss: 0.4380 - val_acc: 0.8716\n","Epoch 60/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.4985 - acc: 0.8270 - val_loss: 0.3466 - val_acc: 0.8987\n","Epoch 61/80\n","546/546 [==============================] - 282s 517ms/step - loss: 0.4912 - acc: 0.8305 - val_loss: 0.3402 - val_acc: 0.8999\n","Epoch 62/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.4895 - acc: 0.8299 - val_loss: 0.3815 - val_acc: 0.8910\n","Epoch 63/80\n","546/546 [==============================] - 282s 517ms/step - loss: 0.4889 - acc: 0.8309 - val_loss: 0.4019 - val_acc: 0.8867\n","Epoch 64/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.4857 - acc: 0.8299 - val_loss: 0.4226 - val_acc: 0.8771\n","Epoch 65/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.4821 - acc: 0.8320 - val_loss: 0.3950 - val_acc: 0.8859\n","Epoch 66/80\n","546/546 [==============================] - 279s 510ms/step - loss: 0.4788 - acc: 0.8340 - val_loss: 0.3753 - val_acc: 0.8892\n","Epoch 67/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.4796 - acc: 0.8319 - val_loss: 0.3707 - val_acc: 0.8920\n","Epoch 68/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.4773 - acc: 0.8333 - val_loss: 0.3695 - val_acc: 0.8936\n","Epoch 69/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.4745 - acc: 0.8343 - val_loss: 0.3389 - val_acc: 0.9012\n","Epoch 70/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.4761 - acc: 0.8357 - val_loss: 0.4503 - val_acc: 0.8729\n","Epoch 71/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.4678 - acc: 0.8360 - val_loss: 0.3330 - val_acc: 0.9030\n","Epoch 72/80\n","546/546 [==============================] - 282s 516ms/step - loss: 0.4743 - acc: 0.8349 - val_loss: 0.3462 - val_acc: 0.9016\n","Epoch 73/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.4663 - acc: 0.8369 - val_loss: 0.3913 - val_acc: 0.8885\n","Epoch 74/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.4639 - acc: 0.8385 - val_loss: 0.3779 - val_acc: 0.8937\n","Epoch 75/80\n","546/546 [==============================] - 281s 515ms/step - loss: 0.4655 - acc: 0.8389 - val_loss: 0.3628 - val_acc: 0.8978\n","Epoch 76/80\n","546/546 [==============================] - 281s 514ms/step - loss: 0.4625 - acc: 0.8381 - val_loss: 0.3701 - val_acc: 0.8938\n","Epoch 77/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.4616 - acc: 0.8395 - val_loss: 0.3691 - val_acc: 0.8942\n","Epoch 78/80\n","546/546 [==============================] - 278s 509ms/step - loss: 0.4605 - acc: 0.8402 - val_loss: 0.3661 - val_acc: 0.8979\n","Epoch 79/80\n","546/546 [==============================] - 278s 510ms/step - loss: 0.4597 - acc: 0.8396 - val_loss: 0.3587 - val_acc: 0.9014\n","Epoch 80/80\n","546/546 [==============================] - 279s 511ms/step - loss: 0.4573 - acc: 0.8392 - val_loss: 0.3202 - val_acc: 0.9053\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f3e3fd5e0b8>"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"ZcWydmIVhZGr","colab_type":"code","outputId":"de795643-7bf2-4a76-8c5a-4e69efd2e6b7","executionInfo":{"status":"ok","timestamp":1553804416314,"user_tz":-330,"elapsed":21927911,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Save the trained weights in to .h5 format\n","model.save(\"./DNST_model_1a.h5\")\n","print(\"Saved model to disk\")\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]},{"metadata":{"id":"ai-yZ2ED5AK1","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","\n","files.download('./DNST_model_1a.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sgs59HOJSeBI","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save_weights('DNST_model_1a_wt.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nl0C8axuop19","colab_type":"code","colab":{}},"cell_type":"code","source":["files.download('DNST_model_1a_wt.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GDh3_9WRocR4","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import load_model\n","new_model = load_model('DNST_model_1a.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XAcKdtPQp84d","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n","import numpy as np\n","\n","lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n","                                    cooldown=0, patience=5, min_lr=1e-5)\n","model_checkpoint= ModelCheckpoint('DNST_model_1a_wt.h5', monitor=\"val_acc\", save_best_only=True,\n","                                  save_weights_only=True, verbose=1)\n","\n","callbacks=[lr_reducer, model_checkpoint]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NahhFq_OR3vU","colab_type":"code","outputId":"ba0e941e-09a1-4ca7-962a-9bcae639ff6d","colab":{"base_uri":"https://localhost:8080/","height":6048},"executionInfo":{"status":"error","timestamp":1553862905385,"user_tz":-330,"elapsed":1792499,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}}},"cell_type":"code","source":["new_model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                    steps_per_epoch= x_train.shape[0] // batch_size, epochs=120,\n","                    callbacks=callbacks,\n","                    validation_data = (x_test, y_test),\n","                    validation_steps = x_test.shape[0] // batch_size, verbose=1)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Epoch 1/120\n","546/546 [==============================] - 299s 547ms/step - loss: 0.5155 - acc: 0.8221 - val_loss: 0.3819 - val_acc: 0.8916\n","\n","Epoch 00001: val_acc improved from -inf to 0.89160, saving model to DNST_model_1a_wt.h5\n","Epoch 2/120\n","546/546 [==============================] - 291s 533ms/step - loss: 0.5099 - acc: 0.8235 - val_loss: 0.3371 - val_acc: 0.9010\n","\n","Epoch 00002: val_acc improved from 0.89160 to 0.90100, saving model to DNST_model_1a_wt.h5\n","Epoch 3/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.5037 - acc: 0.8247 - val_loss: 0.3402 - val_acc: 0.9055\n","\n","Epoch 00003: val_acc improved from 0.90100 to 0.90550, saving model to DNST_model_1a_wt.h5\n","Epoch 4/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4955 - acc: 0.8285 - val_loss: 0.3428 - val_acc: 0.8993\n","\n","Epoch 00004: val_acc did not improve from 0.90550\n","Epoch 5/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4959 - acc: 0.8285 - val_loss: 0.3349 - val_acc: 0.8998\n","\n","Epoch 00005: val_acc did not improve from 0.90550\n","Epoch 6/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4957 - acc: 0.8281 - val_loss: 0.3415 - val_acc: 0.9023\n","\n","Epoch 00006: val_acc did not improve from 0.90550\n","Epoch 7/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4912 - acc: 0.8298 - val_loss: 0.3563 - val_acc: 0.8975\n","\n","Epoch 00007: val_acc did not improve from 0.90550\n","Epoch 8/120\n","546/546 [==============================] - 287s 525ms/step - loss: 0.4891 - acc: 0.8306 - val_loss: 0.3215 - val_acc: 0.9048\n","\n","Epoch 00008: val_acc did not improve from 0.90550\n","Epoch 9/120\n","546/546 [==============================] - 287s 525ms/step - loss: 0.4692 - acc: 0.8379 - val_loss: 0.3208 - val_acc: 0.9090\n","\n","Epoch 00009: val_acc improved from 0.90550 to 0.90900, saving model to DNST_model_1a_wt.h5\n","Epoch 10/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4643 - acc: 0.8381 - val_loss: 0.3171 - val_acc: 0.9082\n","\n","Epoch 00010: val_acc did not improve from 0.90900\n","Epoch 11/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4672 - acc: 0.8383 - val_loss: 0.3075 - val_acc: 0.9105\n","\n","Epoch 00011: val_acc improved from 0.90900 to 0.91050, saving model to DNST_model_1a_wt.h5\n","Epoch 12/120\n","546/546 [==============================] - 287s 525ms/step - loss: 0.4609 - acc: 0.8396 - val_loss: 0.3156 - val_acc: 0.9072\n","\n","Epoch 00012: val_acc did not improve from 0.91050\n","Epoch 13/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4644 - acc: 0.8384 - val_loss: 0.3095 - val_acc: 0.9093\n","\n","Epoch 00013: val_acc did not improve from 0.91050\n","Epoch 14/120\n","546/546 [==============================] - 286s 524ms/step - loss: 0.4629 - acc: 0.8388 - val_loss: 0.3145 - val_acc: 0.9075\n","\n","Epoch 00014: val_acc did not improve from 0.91050\n","Epoch 15/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4616 - acc: 0.8390 - val_loss: 0.3226 - val_acc: 0.9071\n","\n","Epoch 00015: val_acc did not improve from 0.91050\n","Epoch 16/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4618 - acc: 0.8404 - val_loss: 0.3109 - val_acc: 0.9106\n","\n","Epoch 00016: val_acc improved from 0.91050 to 0.91060, saving model to DNST_model_1a_wt.h5\n","Epoch 17/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4506 - acc: 0.8442 - val_loss: 0.3174 - val_acc: 0.9093\n","\n","Epoch 00017: val_acc did not improve from 0.91060\n","Epoch 18/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4475 - acc: 0.8439 - val_loss: 0.3087 - val_acc: 0.9102\n","\n","Epoch 00018: val_acc did not improve from 0.91060\n","Epoch 19/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4532 - acc: 0.8437 - val_loss: 0.3140 - val_acc: 0.9086\n","\n","Epoch 00019: val_acc did not improve from 0.91060\n","Epoch 20/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4516 - acc: 0.8428 - val_loss: 0.3100 - val_acc: 0.9091\n","\n","Epoch 00020: val_acc did not improve from 0.91060\n","Epoch 21/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4520 - acc: 0.8444 - val_loss: 0.3044 - val_acc: 0.9106\n","\n","Epoch 00021: val_acc did not improve from 0.91060\n","Epoch 22/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4495 - acc: 0.8428 - val_loss: 0.3081 - val_acc: 0.9095\n","\n","Epoch 00022: val_acc did not improve from 0.91060\n","Epoch 23/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4526 - acc: 0.8418 - val_loss: 0.3117 - val_acc: 0.9089\n","\n","Epoch 00023: val_acc did not improve from 0.91060\n","Epoch 24/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4500 - acc: 0.8429 - val_loss: 0.3097 - val_acc: 0.9086\n","\n","Epoch 00024: val_acc did not improve from 0.91060\n","Epoch 25/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4509 - acc: 0.8442 - val_loss: 0.3101 - val_acc: 0.9093\n","\n","Epoch 00025: val_acc did not improve from 0.91060\n","Epoch 26/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4516 - acc: 0.8441 - val_loss: 0.3118 - val_acc: 0.9091\n","\n","Epoch 00026: val_acc did not improve from 0.91060\n","Epoch 27/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4438 - acc: 0.8451 - val_loss: 0.3097 - val_acc: 0.9088\n","\n","Epoch 00027: val_acc did not improve from 0.91060\n","Epoch 28/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4514 - acc: 0.8426 - val_loss: 0.3111 - val_acc: 0.9091\n","\n","Epoch 00028: val_acc did not improve from 0.91060\n","Epoch 29/120\n","546/546 [==============================] - 287s 527ms/step - loss: 0.4478 - acc: 0.8443 - val_loss: 0.3094 - val_acc: 0.9091\n","\n","Epoch 00029: val_acc did not improve from 0.91060\n","Epoch 30/120\n","546/546 [==============================] - 287s 527ms/step - loss: 0.4439 - acc: 0.8452 - val_loss: 0.3095 - val_acc: 0.9093\n","\n","Epoch 00030: val_acc did not improve from 0.91060\n","Epoch 31/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4464 - acc: 0.8454 - val_loss: 0.3110 - val_acc: 0.9090\n","\n","Epoch 00031: val_acc did not improve from 0.91060\n","Epoch 32/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4510 - acc: 0.8434 - val_loss: 0.3114 - val_acc: 0.9089\n","\n","Epoch 00032: val_acc did not improve from 0.91060\n","Epoch 33/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4520 - acc: 0.8429 - val_loss: 0.3112 - val_acc: 0.9090\n","\n","Epoch 00033: val_acc did not improve from 0.91060\n","Epoch 34/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4459 - acc: 0.8451 - val_loss: 0.3115 - val_acc: 0.9090\n","\n","Epoch 00034: val_acc did not improve from 0.91060\n","Epoch 35/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4466 - acc: 0.8450 - val_loss: 0.3114 - val_acc: 0.9087\n","\n","Epoch 00035: val_acc did not improve from 0.91060\n","Epoch 36/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4455 - acc: 0.8451 - val_loss: 0.3106 - val_acc: 0.9090\n","\n","Epoch 00036: val_acc did not improve from 0.91060\n","Epoch 37/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4457 - acc: 0.8469 - val_loss: 0.3108 - val_acc: 0.9089\n","\n","Epoch 00037: val_acc did not improve from 0.91060\n","Epoch 38/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4505 - acc: 0.8421 - val_loss: 0.3107 - val_acc: 0.9090\n","\n","Epoch 00038: val_acc did not improve from 0.91060\n","Epoch 39/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4520 - acc: 0.8432 - val_loss: 0.3109 - val_acc: 0.9094\n","\n","Epoch 00039: val_acc did not improve from 0.91060\n","Epoch 40/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4441 - acc: 0.8450 - val_loss: 0.3102 - val_acc: 0.9095\n","\n","Epoch 00040: val_acc did not improve from 0.91060\n","Epoch 41/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4483 - acc: 0.8442 - val_loss: 0.3093 - val_acc: 0.9094\n","\n","Epoch 00041: val_acc did not improve from 0.91060\n","Epoch 42/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4498 - acc: 0.8449 - val_loss: 0.3093 - val_acc: 0.9091\n","\n","Epoch 00042: val_acc did not improve from 0.91060\n","Epoch 43/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4501 - acc: 0.8443 - val_loss: 0.3111 - val_acc: 0.9086\n","\n","Epoch 00043: val_acc did not improve from 0.91060\n","Epoch 44/120\n","546/546 [==============================] - 287s 527ms/step - loss: 0.4431 - acc: 0.8458 - val_loss: 0.3100 - val_acc: 0.9092\n","\n","Epoch 00044: val_acc did not improve from 0.91060\n","Epoch 45/120\n","546/546 [==============================] - 287s 527ms/step - loss: 0.4483 - acc: 0.8443 - val_loss: 0.3089 - val_acc: 0.9092\n","\n","Epoch 00045: val_acc did not improve from 0.91060\n","Epoch 46/120\n","546/546 [==============================] - 287s 526ms/step - loss: 0.4482 - acc: 0.8444 - val_loss: 0.3087 - val_acc: 0.9097\n","\n","Epoch 00046: val_acc did not improve from 0.91060\n","Epoch 47/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4506 - acc: 0.8432 - val_loss: 0.3089 - val_acc: 0.9092\n","\n","Epoch 00047: val_acc did not improve from 0.91060\n","Epoch 48/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4472 - acc: 0.8438 - val_loss: 0.3085 - val_acc: 0.9092\n","\n","Epoch 00048: val_acc did not improve from 0.91060\n","Epoch 49/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4512 - acc: 0.8426 - val_loss: 0.3100 - val_acc: 0.9089\n","\n","Epoch 00049: val_acc did not improve from 0.91060\n","Epoch 50/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4527 - acc: 0.8417 - val_loss: 0.3097 - val_acc: 0.9092\n","\n","Epoch 00050: val_acc did not improve from 0.91060\n","Epoch 51/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4460 - acc: 0.8451 - val_loss: 0.3111 - val_acc: 0.9093\n","\n","Epoch 00051: val_acc did not improve from 0.91060\n","Epoch 52/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4476 - acc: 0.8445 - val_loss: 0.3105 - val_acc: 0.9092\n","\n","Epoch 00052: val_acc did not improve from 0.91060\n","Epoch 53/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4513 - acc: 0.8435 - val_loss: 0.3102 - val_acc: 0.9097\n","\n","Epoch 00053: val_acc did not improve from 0.91060\n","Epoch 54/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4484 - acc: 0.8423 - val_loss: 0.3096 - val_acc: 0.9092\n","\n","Epoch 00054: val_acc did not improve from 0.91060\n","Epoch 55/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4461 - acc: 0.8444 - val_loss: 0.3089 - val_acc: 0.9095\n","\n","Epoch 00055: val_acc did not improve from 0.91060\n","Epoch 56/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4464 - acc: 0.8452 - val_loss: 0.3093 - val_acc: 0.9095\n","\n","Epoch 00056: val_acc did not improve from 0.91060\n","Epoch 57/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4511 - acc: 0.8439 - val_loss: 0.3093 - val_acc: 0.9090\n","\n","Epoch 00057: val_acc did not improve from 0.91060\n","Epoch 58/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4505 - acc: 0.8434 - val_loss: 0.3096 - val_acc: 0.9091\n","\n","Epoch 00058: val_acc did not improve from 0.91060\n","Epoch 59/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4471 - acc: 0.8458 - val_loss: 0.3112 - val_acc: 0.9096\n","\n","Epoch 00059: val_acc did not improve from 0.91060\n","Epoch 60/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4460 - acc: 0.8433 - val_loss: 0.3092 - val_acc: 0.9092\n","\n","Epoch 00060: val_acc did not improve from 0.91060\n","Epoch 61/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4485 - acc: 0.8443 - val_loss: 0.3099 - val_acc: 0.9091\n","\n","Epoch 00061: val_acc did not improve from 0.91060\n","Epoch 62/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4513 - acc: 0.8442 - val_loss: 0.3101 - val_acc: 0.9089\n","\n","Epoch 00062: val_acc did not improve from 0.91060\n","Epoch 63/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4449 - acc: 0.8457 - val_loss: 0.3090 - val_acc: 0.9094\n","\n","Epoch 00063: val_acc did not improve from 0.91060\n","Epoch 64/120\n","546/546 [==============================] - 288s 527ms/step - loss: 0.4474 - acc: 0.8447 - val_loss: 0.3102 - val_acc: 0.9099\n","\n","Epoch 00064: val_acc did not improve from 0.91060\n","Epoch 65/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4472 - acc: 0.8432 - val_loss: 0.3103 - val_acc: 0.9099\n","\n","Epoch 00065: val_acc did not improve from 0.91060\n","Epoch 66/120\n","546/546 [==============================] - 288s 528ms/step - loss: 0.4485 - acc: 0.8452 - val_loss: 0.3102 - val_acc: 0.9097\n","\n","Epoch 00066: val_acc did not improve from 0.91060\n","Epoch 67/120\n","546/546 [==============================] - 289s 529ms/step - loss: 0.4469 - acc: 0.8452 - val_loss: 0.3103 - val_acc: 0.9091\n","\n","Epoch 00067: val_acc did not improve from 0.91060\n","Epoch 68/120\n","546/546 [==============================] - 289s 529ms/step - loss: 0.4420 - acc: 0.8463 - val_loss: 0.3087 - val_acc: 0.9093\n","\n","Epoch 00068: val_acc did not improve from 0.91060\n","Epoch 69/120\n","546/546 [==============================] - 290s 531ms/step - loss: 0.4461 - acc: 0.8445 - val_loss: 0.3104 - val_acc: 0.9098\n","\n","Epoch 00069: val_acc did not improve from 0.91060\n","Epoch 70/120\n","546/546 [==============================] - 290s 531ms/step - loss: 0.4474 - acc: 0.8444 - val_loss: 0.3085 - val_acc: 0.9098\n","\n","Epoch 00070: val_acc did not improve from 0.91060\n","Epoch 71/120\n","546/546 [==============================] - 290s 531ms/step - loss: 0.4514 - acc: 0.8430 - val_loss: 0.3087 - val_acc: 0.9097\n","\n","Epoch 00071: val_acc did not improve from 0.91060\n","Epoch 72/120\n","546/546 [==============================] - 290s 532ms/step - loss: 0.4466 - acc: 0.8448 - val_loss: 0.3089 - val_acc: 0.9097\n","\n","Epoch 00072: val_acc did not improve from 0.91060\n","Epoch 73/120\n","546/546 [==============================] - 290s 532ms/step - loss: 0.4476 - acc: 0.8439 - val_loss: 0.3094 - val_acc: 0.9094\n","\n","Epoch 00073: val_acc did not improve from 0.91060\n","Epoch 74/120\n"," 45/546 [=>............................] - ETA: 4:15 - loss: 0.4630 - acc: 0.8359"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-1eb47e1fb7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_steps = x_test.shape[0] // batch_size, verbose=1)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"zEq88T_XBB8_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5838c11a-2ce8-4695-821f-32729dd1ee65","executionInfo":{"status":"ok","timestamp":1553862912521,"user_tz":-330,"elapsed":2594,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}}},"cell_type":"code","source":["# Save the trained weights in to .h5 format\n","new_model.save(\"./DNST_model_1b.h5\")\n","print(\"Saved model to disk\")"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]},{"metadata":{"id":"Z5B1a8WqBLiO","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","\n","files.download('./DNST_model_1b.h5')\n","files.download('DNST_model_1a_wt.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4F5c1fJXQPu5","colab_type":"code","colab":{}},"cell_type":"code","source":["new_model.save_weights(\"DNST_model_1b_wt.h5\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JhKB5ZJScsx7","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","final = load_model('./DNST_model_1b.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_rRc0tqpAemO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"7b75b4d0-5f84-4d2e-d614-5458aa3998c1","executionInfo":{"status":"ok","timestamp":1553863065868,"user_tz":-330,"elapsed":22031,"user":{"displayName":"herman menezes","photoUrl":"","userId":"14902189756388715287"}}},"cell_type":"code","source":["# Test the model\n","score = final.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":56,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 16s 2ms/step\n","Test loss: 0.3093938493475318\n","Test accuracy: 0.909\n"],"name":"stdout"}]},{"metadata":{"id":"02SrEK9vUJmw","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}